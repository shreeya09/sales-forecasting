{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreeya09/sales-forecasting/blob/main/Walmart_Sales_Forecasting_(Mid_Course_Assessment_ML_Project).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s competitive retail environment, data-driven decision-making is no longer optional — it is a necessity. This project, Walmart Sales Forecasting, aims to use historical sales data to build machine learning models that can accurately predict future weekly sales. Forecasting sales not only enables better inventory and supply chain management but also ensures optimal staffing, marketing, and operational planning. Walmart, being a retail giant with a complex sales ecosystem across numerous stores and departments, presents a rich and challenging dataset ideal for predictive analytics.\n",
        "\n",
        "The core goal of this project is to develop a regression model that predicts the weekly sales for Walmart stores using historical data. The dataset provided contains features such as store number, department number, date, whether a given week is a holiday, and the weekly sales figure. In addition to these, temporal features like the month, year, and ISO calendar week are extracted to enrich the feature space and capture seasonal and cyclical trends.\n",
        "\n",
        "**The project is structured in several phases:**\n",
        "\n",
        "**Problem Definition:**\n",
        "The objective is to build a machine learning model that can predict the Weekly_Sales value using relevant features. The problem type is regression, as the output is a continuous numerical value.\n",
        "\n",
        "**Data Exploration and Cleaning:**\n",
        "The dataset is loaded and examined for structure, missing values, and duplicate entries. We convert the date column to a datetime object and derive new time-based features such as Month, Week, and Year. The IsHoliday field is encoded into binary (0 or 1), and missing values are forward-filled. Duplicate rows are checked and addressed, and all features are examined for their unique values and distributions.\n",
        "\n",
        "**Exploratory Data Analysis (EDA):**\n",
        "This phase involves visualizing and understanding relationships between variables. Line plots are used to analyze trends in total weekly sales over time. Box plots show variability across different stores. Holiday vs. non-holiday sales are compared using bar charts, revealing the impact of seasonal events. A heatmap is also generated to explore correlations among numerical features. From these visualizations, patterns like higher sales during holiday seasons, monthly seasonality, and store-wise performance disparities are uncovered.\n",
        "\n",
        "**Hypothesis Testing:**\n",
        "Before modeling, we formulate and consider several hypotheses such as:\n",
        "\n",
        "“Holiday weeks yield significantly higher sales than non-holiday weeks.”\n",
        "\n",
        "“Sales remain normally distributed over time.”\n",
        "\n",
        "“Department-wise sales show consistent behavior annually.”\n",
        "\n",
        "These hypotheses guide feature selection and model expectations.\n",
        "\n",
        "**Feature Engineering:**\n",
        "Important features like store ID, department number, holiday flag, and time features are selected. These features are used to train machine learning models. Proper splitting of the data into training and testing sets ensures robust evaluation.\n",
        "\n",
        "**Modeling and Evaluation:**\n",
        "Multiple machine learning models are implemented — Linear Regression, Decision Tree Regressor, and Random Forest Regressor. Each model is evaluated using regression metrics: Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared. Among these, the Random Forest model outperforms the others with the lowest error rates and highest explanatory power."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic and competitive landscape of retail, accurately forecasting sales is crucial for effective inventory management, staffing, supply chain optimization, and financial planning. Walmart, as one of the largest retail chains in the world, generates a massive volume of sales data across hundreds of stores and departments. Making sense of this data to forecast future weekly sales is both a challenge and an opportunity.\n",
        "\n",
        "The goal of this project is to develop a machine learning model that can predict the weekly sales for Walmart stores using historical data. The dataset includes key attributes such as store ID, department number, date, whether the week is a holiday, and the actual weekly sales figures. These variables provide the foundation for uncovering patterns, seasonality, and trends in sales behavior.\n",
        "\n",
        "This is a supervised regression problem, where the target variable Weekly_Sales is continuous, and the objective is to minimize prediction errors on unseen data. By leveraging historical sales patterns and other relevant features, we aim to create a predictive model that can assist Walmart in making informed decisions for future weeks.\n",
        "\n",
        "The solution should:\n",
        "\n",
        "-Accurately forecast weekly sales based on available data.\n",
        "\n",
        "-Capture the effects of holidays and seasonal trends.\n",
        "\n",
        "-Generalize well to new data (stores, weeks, or departments not seen during training).\n",
        "\n",
        "-Provide actionable insights through data analysis and visualization.\n",
        "\n",
        "Ultimately, this model will help Walmart proactively plan logistics, improve product availability, reduce overstock and stockouts, and deliver a better customer experience through smarter forecasting."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Warnings and Display settings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path = 'https://raw.githubusercontent.com/shreeya09/sales-forecasting/main/walmart.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, columns = df.shape\n",
        "print(f\"Number of rows: {rows}\")\n",
        "print(f\"Number of columns: {columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing/Null values in each column:\\n\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Walmart sales dataset contains **6,435 records and 8 columns**, capturing weekly sales performance across 45 stores. Each row represents the sales for a specific store in a particular week.\n",
        "\n",
        "The key target variable is Weekly_Sales, which records the total sales in dollars. The dataset includes features like Store (store ID), Date (week-ending date), Holiday_Flag (binary indicator for major holidays), Temperature (average temperature), Fuel_Price (fuel cost), CPI (Consumer Price Index), and Unemployment (unemployment rate).\n",
        "\n",
        "These features provide insights into store operations, economic conditions, and seasonal trends. Importantly, the dataset has no missing values and no duplicate rows, ensuring data quality is high for analysis and modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Columns\n",
        "print(\"Dataset Columns:\\n\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Store**: This column represents the unique identifier for each Walmart store. There are 45 unique stores in the dataset, indicating that data is collected from 45 different retail locations. This field helps segment sales and performance across geographic and operational variations among the stores.\n",
        "\n",
        "**Date**: This column contains the week-ending date for each sales record. It is a critical feature for time-series analysis and h **bold text**elps in extracting seasonal trends and identifying holiday effects. There are 143 unique dates, implying the data spans roughly three years of weekly observations.\n",
        "\n",
        "**Weekly_Sales**: This is the target variable of the project. It represents the total dollar amount of sales generated by a specific store in a particular week. Each value is continuous and unique to the store-week combination. The goal of the machine learning model is to accurately forecast this variable.\n",
        "\n",
        "**Holiday_Flag**: This binary column indicates whether the week contains a major holiday (e.g., Super Bowl, Labor Day, Thanksgiving, or Christmas). A value of 1 means it's a holiday week, and 0 otherwise. This flag is essential for understanding the impact of holidays on consumer purchasing behavior.\n",
        "\n",
        "**Temperature**: This field shows the average temperature for that week in the respective store's location. Weather can influence store traffic and shopping patterns, so it’s useful to analyze its correlation with sales.\n",
        "\n",
        "**Fuel_Price**: This column indicates the cost of fuel during the week, which can affect consumer spending and store visits, particularly in suburban or rural areas. It has 892 unique values, showing moderate variability over time.\n",
        "\n",
        "**CPI (Consumer Price Index**): This economic indicator reflects the average change over time in the prices paid by consumers for goods and services. It can impact overall spending capacity and patterns. This column can help capture macroeconomic influences on retail sales.\n",
        "\n",
        "**Unemployment**: This represents the unemployment rate for the region in that specific week. Higher unemployment might correlate with lower spending. With 349 unique values, this feature may serve as a good economic signal for sales prediction."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "print(\"Unique values in each column:\\n\")\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove duplicates if any\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 2. Handle missing values\n",
        "# (There were none in your dataset, but this is a safety net)\n",
        "df = df.fillna(method='ffill')\n",
        "\n",
        "# 3. Convert 'Date' column to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# 4. Create time-based features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "# 5. Ensure 'Holiday_Flag' is integer (for modeling)\n",
        "df['Holiday_Flag'] = df['Holiday_Flag'].astype(int)\n",
        "\n",
        "# 6. Re-check data types\n",
        "print(df.dtypes)\n",
        "\n",
        "# 7. Final shape and sample\n",
        "print(f\"Cleaned Dataset Shape: {df.shape}\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Removed any duplicate rows\n",
        "\n",
        "-Forward-filled missing values (if any)\n",
        "\n",
        "-Converted the Date column to datetime\n",
        "\n",
        "-Extracted Year, Month, Week for time-based analysis\n",
        "\n",
        "-Converted Holiday_Flag to numeric (if it wasn’t already)\n",
        "\n",
        "**Insights Found So Far**\n",
        "\n",
        "-Clean and Complete Data: The dataset is free of missing and duplicate values, which means it's reliable for modeling and doesn't require imputation or heavy cleaning.\n",
        "\n",
        "-Time-Aware Data: Creating new time-based features (like Month, Year, Week) allows for the detection of seasonal trends, such as spikes in sales during holidays or end-of-year periods.\n",
        "\n",
        "-Holiday Indicator: The binary Holiday_Flag column will be valuable for evaluating sales performance during major holidays, which is common in retail forecasting.\n",
        "\n",
        "-Economic Context: Columns like Fuel_Price, CPI, and Unemployment introduce macroeconomic factors that could influence purchasing power and consumer behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Weekly Sales Over Time"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Weekly Sales Over Time\n",
        "df.groupby('Date')['Weekly_Sales'].sum().plot(figsize=(14, 5), title=\"1. Total Weekly Sales Over Time\")\n",
        "plt.ylabel(\"Sales ($)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gained Insights and Business Impact"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**:\n",
        "\n",
        "The line chart of total weekly sales shows clear fluctuations over time, with noticeable spikes during holiday seasons and dips during off-peak periods.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "Helps Walmart anticipate demand surges and prepare by increasing inventory, staff, and marketing during high-sales weeks.\n",
        "\n",
        "Useful for seasonal planning and understanding year-over-year growth or decline."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2- Average Sales by Month"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Average Sales by Month\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Month', y='Weekly_Sales', data=df, estimator=np.mean)\n",
        "plt.title(\"2. Average Weekly Sales by Month\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Average Sales ($)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gained Insights and Business Impact"
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**:\n",
        "\n",
        "This bar chart reveals that certain months consistently outperform others in terms of sales (e.g., November and December often show higher averages due to holidays).\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "Assists in budgeting and promotional planning based on monthly trends.\n",
        "\n",
        "Helps optimize supply chain operations by planning restocking and logistics ahead of high-sales months.\n",
        "\n",
        "Drives strategic campaign timing for sales and discounts."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Holiday vs Non-Holiday Sales"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Holiday vs Non-Holiday Sales\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.boxplot(x='Holiday_Flag', y='Weekly_Sales', data=df)\n",
        "plt.title(\"3. Sales Distribution: Holiday vs Non-Holiday Weeks\")\n",
        "plt.xticks([0, 1], ['Non-Holiday', 'Holiday'])\n",
        "plt.ylabel(\"Weekly Sales ($)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gained Insights and Business Impact"
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**:\n",
        "\n",
        "Sales during holiday weeks are generally higher and more variable than during non-holiday weeks, as shown by the box plot.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "Justifies investing in holiday-specific promotions and staffing boosts.\n",
        "\n",
        "Enables precise forecasting adjustments for special weeks.\n",
        "\n",
        "Supports holiday-centric stock management to avoid shortages or overstocking."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Average Sales by Store"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Average Sales by Store\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "avg_sales = df.groupby('Store')['Weekly_Sales'].mean().sort_values()\n",
        "sns.barplot(x=avg_sales.index, y=avg_sales.values)\n",
        "plt.title(\"4. Average Weekly Sales by Store\")\n",
        "plt.xlabel(\"Store\")\n",
        "plt.ylabel(\"Average Sales ($)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gained Insights and Business Impact"
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**:\n",
        "\n",
        "There’s significant variation in performance between stores — some consistently outperform others, while a few lag behind.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "Identifies high-performing stores to replicate their strategies across the chain.\n",
        "\n",
        "Flags underperforming stores for audit, retraining, or regional analysis.\n",
        "\n",
        "Helps in resource allocation, i.e., prioritizing investment and logistics where ROI is highest."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"5. Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**:\n",
        "\n",
        "The correlation matrix shows how variables like CPI, Fuel Price, and Temperature relate (positively or negatively) to weekly sales. For example, temperature may be mildly correlated, while CPI and unemployment are more stable.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "Aids in feature selection for machine learning models, improving prediction accuracy.\n",
        "\n",
        "Reveals external economic drivers that affect consumer spending.\n",
        "\n",
        "Allows Walmart to consider economic indicators in strategic planning."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "# Select important numeric columns\n",
        "selected_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "\n",
        "# Create pair plot\n",
        "sns.pairplot(df[selected_cols], corner=True)\n",
        "plt.suptitle(\"Pair Plot of Key Variables\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals relationships and distribution patterns between multiple variables at once.\n",
        "\n",
        "Helps identify linear or non-linear trends, clusters, or outliers.\n",
        "\n",
        "Useful for feature engineering and initial hypothesis testing before model training."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1:**\n",
        "Holiday weeks have significantly higher weekly sales than non-holiday weeks.\n",
        "We observed this in the boxplot comparing holiday vs non-holiday sales.\n",
        "\n",
        "**Hypothesis 2:**\n",
        "There is a significant difference in average sales across different months.\n",
        "Monthly bar chart showed seasonality in sales — we’ll test if this is statistically significant.\n",
        "\n",
        "**Hypothesis 3:**\n",
        "Fuel prices are correlated with weekly sales.\n",
        "From the heatmap and pair plot, we suspect there may be a relationship worth testing."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null (H₀): Mean sales during holiday weeks = Mean sales during non-holiday weeks\n",
        "\n",
        "Alt (H₁): Mean sales during holiday weeks ≠ Mean sales during non-holiday weeks"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind, f_oneway, pearsonr\n",
        "\n",
        "holiday_sales = df[df['Holiday_Flag'] == 1]['Weekly_Sales']\n",
        "non_holiday_sales = df[df['Holiday_Flag'] == 0]['Weekly_Sales']\n",
        "t_stat, p_val_ttest = ttest_ind(holiday_sales, non_holiday_sales, equal_var=False)\n",
        "print(\"Hypothesis 1 - t-test (Holiday vs Non-Holiday Sales):\")\n",
        "print(f\"  t-statistic = {t_stat:.3f}, p-value = {p_val_ttest:.5f}\")\n",
        "print(\"  → Reject H0 if p < 0.05\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Independent Samples t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-test is used to compare the means of two independent groups. In this case, the two groups are:\n",
        "\n",
        "Weekly sales during holiday weeks\n",
        "\n",
        "Weekly sales during non-holiday weeks\n",
        "\n",
        "Since the sales values are continuous and the two groups are distinct and non-overlapping, an independent two-sample t-test is the most suitable choice. It helps determine if the difference in average sales between the two types of weeks is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null (H₀): Mean weekly sales are the same across all months\n",
        "\n",
        "Alt (H₁): At least one month has a different mean weekly sales"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "anova_groups = [df[df['Month'] == m]['Weekly_Sales'] for m in df['Month'].unique()]\n",
        "f_stat, p_val_anova = f_oneway(*anova_groups)\n",
        "\n",
        "print(\"Hypothesis 2 - ANOVA (Sales across Months):\")\n",
        "print(f\"  F-statistic = {f_stat:.3f}, p-value = {p_val_anova:.5f}\")\n",
        "print(\"  → Reject H0 if p < 0.05\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: One-Way ANOVA (Analysis of Variance)"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA is used when comparing the means of more than two independent groups. Here, we are analyzing average sales across 12 months, which are separate groups (January to December).\n",
        "\n",
        "ANOVA checks whether at least one of these groups has a mean that's significantly different from the others. It's ideal for detecting seasonal variations in sales across the year.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null (H₀): There is no correlation between fuel price and weekly sales\n",
        "\n",
        "Alt (H₁): There is a statistically significant correlation between fuel price and weekly sales"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "corr_coef, p_val_corr = pearsonr(df['Fuel_Price'], df['Weekly_Sales'])\n",
        "\n",
        "print(\"Hypothesis 3 - Pearson Correlation (Fuel Price vs Sales):\")\n",
        "print(f\"  Correlation = {corr_coef:.3f}, p-value = {p_val_corr:.5f}\")\n",
        "print(\"  → Significant if p < 0.05\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Pearson Correlation Coefficient"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson correlation measures the linear relationship between two continuous variables — in this case:\n",
        "\n",
        "Fuel_Price (independent variable)\n",
        "\n",
        "Weekly_Sales (dependent variable)\n",
        "\n",
        "It provides a correlation coefficient (ranging from -1 to +1) and a p-value to determine whether the relationship is statistically significant. This test is appropriate because both variables are numerical and assumed to be normally distributed."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Optional: Remove extreme sales outliers using IQR\n",
        "Q1 = df['Weekly_Sales'].quantile(0.25)\n",
        "Q3 = df['Weekly_Sales'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[(df['Weekly_Sales'] >= Q1 - 1.5 * IQR) & (df['Weekly_Sales'] <= Q3 + 1.5 * IQR)]\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier treatment was considered due to the presence of extreme spikes in weekly sales, particularly during holidays. Although not removed by default to preserve meaningful seasonal patterns, we used the Interquartile Range (IQR) method as an optional approach to identify and filter extreme sales values when necessary."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Convert 'Date' to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract time-based features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "# Encode Holiday flag as binary\n",
        "df['Holiday_Flag'] = df['Holiday_Flag'].astype(int)\n",
        "\n",
        "# Optionally: Create 'Season' feature\n",
        "df['Season'] = df['Month'] % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Fall\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Define feature columns and target variable\n",
        "features = ['Store', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI',\n",
        "            'Unemployment', 'Month', 'Week', 'Year', 'Season']\n",
        "target = 'Weekly_Sales'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For feature selection, we relied primarily on manual selection driven by domain knowledge and insights gained during exploratory data analysis (EDA)."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features like Store, Holiday_Flag, Temperature, Fuel_Price, CPI, Unemployment, and date-derived variables such as Month, Week, and Year were retained due to their logical relationship with sales behavior. These features are not only interpretable but also supported by visual evidence of their influence on sales trends."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Transformation May Be Needed:**\n",
        "\n",
        "-The Weekly_Sales values often have:\n",
        "-Skewed distribution with very high peaks during holidays\n",
        "\n",
        "Large scale (in the range of hundreds of thousands or millions), which may affect model stability — especially for linear models or algorithms sensitive to magnitude and variance.\n",
        "\n",
        "**Why Use the following transformation?**\n",
        "-Reduces right skewness in sales data\n",
        "-Compresses extreme values, making the distribution closer to normal\n",
        "-Helps improve performance of models like Linear Regression, Ridge/Lasso, and algorithms assuming Gaussian distribution\n",
        "-Maintains all values as real and interpretable"
      ],
      "metadata": {
        "id": "cobFkMopujzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "df['Weekly_Sales_log'] = np.log1p(df['Weekly_Sales'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "The dataset was scaled using StandardScaler from scikit-learn, which standardizes features by removing the mean and scaling to unit variance. This was important for models like linear regression that are sensitive to feature magnitude. We did not apply dimensionality reduction, as the number of features was relatively small and all variables were meaningful and relevant"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did not apply dimensionality reduction, as the number of features was relatively small and all variables were meaningful and relevant. Introducing dimensionality reduction like PCA would risk losing interpretability without a significant gain in performance."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For data splitting, we used an 80/20 train-test ratio, a widely accepted standard that balances training performance with the ability to evaluate generalization on unseen data. This split ensures enough data for learning while holding back sufficient data for reliable testing of model performance."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np  # Also import numpy\n",
        "\n",
        "# Scaling for linear regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "# Correct evaluation without 'squared' argument\n",
        "mse = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse = np.sqrt(mse)  # Take square root manually for RMSE\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred_lr))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your Evaluation Metric Scores for Linear Regression\n",
        "rmse_lr = 519075.10\n",
        "mae_lr = 431692.64\n",
        "r2_lr = 0.164\n",
        "\n",
        "# Metrics and Values\n",
        "metrics = ['RMSE', 'MAE', 'R² Score']\n",
        "scores = [rmse_lr, mae_lr, r2_lr]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "\n",
        "# Adding value labels on top\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('📈 Linear Regression Model Evaluation Metrics')\n",
        "plt.ylabel('Score Value')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Scaling features if not already done\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 50, 100]  # Regularization strength\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid,\n",
        "                           cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_ridge = best_ridge.predict(X_test_scaled)\n",
        "\n",
        "# 📊 Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred_ridge)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
        "r2 = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Score (Negative MSE):\", grid_search.best_score_)\n",
        "print(\"\\nTuned Ridge Regression Model Performance:\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization.\n",
        "\n",
        "GridSearchCV exhaustively searches through a specified grid of hyperparameter values.\n",
        "\n",
        "It performs k-fold cross-validation (we used 5-fold CV) for each combination of parameters to find the set of parameters that gives the best performance.\n",
        "\n",
        "It is simple, systematic, and guarantees finding the best parameters within the defined grid.\n",
        "\n",
        "GridSearchCV is ideal when:\n",
        "\n",
        "The parameter space is small or manageable (like tuning alpha in Ridge Regression).\n",
        "\n",
        "You want precise, optimal parameters rather than a rough guess.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we saw an improvement!\n",
        "After tuning the Ridge Regression model's hyperparameters using GridSearchCV:\n",
        "\n",
        "RMSE decreased slightly\n",
        "\n",
        "MAE decreased slightly\n",
        "\n",
        "R² Score increased a bit, indicating better explanatory power\n",
        "\n",
        "While the improvement was not drastic (because Ridge is still a linear model and the data is complex), the model's overall fit improved compared to the simple Linear Regression baseline."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Results\")\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "rmse_dt = np.sqrt(mse_dt)  # Take square root manually\n",
        "\n",
        "print(\"RMSE:\", rmse_dt)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred_dt))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "oFaM0jpsr1jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Fit the Decision Tree model\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics manually\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "rmse_dt = np.sqrt(mse_dt)\n",
        "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "# Metrics and Values\n",
        "metrics = ['RMSE', 'MAE', 'R² Score']\n",
        "scores = [rmse_dt, mae_dt, r2_dt]\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "\n",
        "# Add value labels on top\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('🌳 Decision Tree Regressor - Evaluation Metrics')\n",
        "plt.ylabel('Score Value')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "# Define parameter grid for Decision Tree\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search_dt = GridSearchCV(estimator=dt_model,\n",
        "                              param_grid=param_grid_dt,\n",
        "                              cv=5,                   # 5-Fold Cross-Validation\n",
        "                              scoring='neg_mean_squared_error',\n",
        "                              verbose=1)\n",
        "\n",
        "# Fit the Grid Search to the data\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# Best model after tuning\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_best_dt = best_dt.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned Decision Tree\n",
        "mse_best_dt = mean_squared_error(y_test, y_pred_best_dt)\n",
        "rmse_best_dt = np.sqrt(mse_best_dt)\n",
        "mae_best_dt = mean_absolute_error(y_test, y_pred_best_dt)\n",
        "r2_best_dt = r2_score(y_test, y_pred_best_dt)\n",
        "\n",
        "# Print results\n",
        "print(\" Best Hyperparameters for Decision Tree:\", grid_search_dt.best_params_)\n",
        "print(\"\\nTuned Decision Tree Performance:\")\n",
        "print(f\"RMSE: {rmse_best_dt:.2f}\")\n",
        "print(f\"MAE: {mae_best_dt:.2f}\")\n",
        "print(f\"R² Score: {r2_best_dt:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which Hyperparameter Optimization Technique Have You Used and Why?\n",
        "We used GridSearchCV for hyperparameter optimization on the Decision Tree Regressor.\n",
        "\n",
        "GridSearchCV systematically tries every combination of given hyperparameters.\n",
        "\n",
        "It uses k-fold cross-validation (we used 5 folds) to evaluate the performance of each combination.\n",
        "\n",
        "It guarantees finding the best performing set of hyperparameters within the defined search space."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we observed a significant improvement after hyperparameter tuning:\n",
        "\n",
        "-RMSE decreased (better predictive accuracy).\n",
        "\n",
        "-MAE decreased (lower average error).\n",
        "\n",
        "-R² increased (better explanatory power — from 93.9% to 94.8%)."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Manual RMSE calculation\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Random Forest Results\")\n",
        "print(\"RMSE:\", rmse_rf)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred_rf))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "# Your Evaluation Metric Scores for Random Forest\n",
        "rmse_rf = 113283.34  # Replace with your calculated RMSE\n",
        "mae_rf = 61764.26    # Replace with your calculated MAE\n",
        "r2_rf = 0.960        # Replace with your calculated R²\n",
        "\n",
        "# Metrics and Values\n",
        "metrics = ['RMSE', 'MAE', 'R² Score']\n",
        "scores = [rmse_rf, mae_rf, r2_rf]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "\n",
        "# Adding value labels on top\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('🌳 Random Forest Model Evaluation Metrics')\n",
        "plt.ylabel('Score Value')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter space\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Randomized Search Setup\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf,\n",
        "                                      param_distributions=param_dist_rf,\n",
        "                                      n_iter=20,             # Search only 20 random combos\n",
        "                                      cv=3,                  # 3-Fold Cross Validation (faster)\n",
        "                                      scoring='neg_mean_squared_error',\n",
        "                                      verbose=1,\n",
        "                                      n_jobs=-1,\n",
        "                                      random_state=42)\n",
        "\n",
        "# Fit the randomized search model\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
        "rmse_best_rf = np.sqrt(mse_best_rf)\n",
        "mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)\n",
        "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
        "\n",
        "print(\"Best Hyperparameters for Random Forest:\", random_search_rf.best_params_)\n",
        "print(\"Tuned Random Forest Performance:\")\n",
        "print(f\"RMSE: {rmse_best_rf:.2f}\")\n",
        "print(f\"MAE: {mae_best_rf:.2f}\")\n",
        "print(f\"R² Score: {r2_best_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used RandomizedSearchCV for hyperparameter optimization of the Random Forest Regressor model.\n",
        "\n",
        "Why RandomizedSearchCV?\n",
        "\n",
        "GridSearchCV was taking too much time because it tries all combinations of parameters (108 combinations × 5-fold CV = 540 models).\n",
        "\n",
        "RandomizedSearchCV, on the other hand, randomly samples a limited number of parameter combinations (for example, 20 random tries), which makes it much faster.\n",
        "\n",
        "It still covers the parameter space efficiently and is good enough to find an almost-optimal model.\n",
        "\n",
        "It is ideal when the parameter grid is large and time or computation resources are limited.\n",
        "\n",
        "Thus, RandomizedSearchCV helped us achieve good performance improvements quickly without exhaustively searching the entire parameter grid.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we observed a clear improvement in model performance after applying RandomizedSearchCV for hyperparameter tuning on the Random Forest Regressor.\n",
        "\n",
        "Before tuning:\n",
        "\n",
        "The Random Forest model used default parameters.\n",
        "\n",
        "Although the model performed well, it was not fully optimized for the data patterns and complexity.\n",
        "\n",
        "After tuning:\n",
        "\n",
        "Using RandomizedSearchCV, we found better hyperparameters (e.g., optimized max_depth, n_estimators, min_samples_split, and min_samples_leaf).\n",
        "\n",
        "The model's predictive performance improved significantly."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Walmart Sales Forecasting project, we consistently used three key evaluation metrics for all three machine learning models (Linear Regression, Decision Tree, Random Forest) to ensure positive business impact:\n",
        "\n",
        "\n",
        "Metric\tPurpose\tBusiness Relevance\n",
        "RMSE (Root Mean Squared Error) measures overall prediction error magnitude\tPenalizes large errors heavily, critical for forecasting large sales numbers.\n",
        "\n",
        "MAE (Mean Absolute Error)\tMeasures average prediction error size\tEasier to interpret in dollars, directly usable for planning inventory and revenue.\n",
        "\n",
        "R² Score (Coefficient of Determination)\tMeasures how much variance in sales is explained by the model\tEnsures good model fit, so management can trust forecasts for decision making."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating all three machine learning models—Linear Regression, Decision Tree Regressor, and Random Forest Regressor—we selected the **Random Forest Regressor** as our final prediction model. This decision was based on its superior performance across all key evaluation metrics.\n",
        "\n",
        "Compared to the other models, Random Forest achieved the lowest Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), along with the highest R² score, indicating it captured the most variance in weekly sales. Unlike Linear Regression, which underperformed due to its inability to model complex patterns, and Decision Trees, which risk overfitting, Random Forest provided a robust balance of accuracy and generalization by averaging the outputs of multiple decision trees.\n",
        "\n",
        "It also effectively handled non-linear relationships, seasonal variations, and holiday impacts in the dataset. Additionally, after applying RandomizedSearchCV for hyperparameter tuning, the model's performance improved further, making it more reliable for real-world forecasting. By choosing Random Forest, we ensure Walmart benefits from more precise sales forecasts, leading to better inventory planning, supply chain optimization, and overall operational efficiency."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we used the Random Forest Regressor as our final prediction model. Random Forest is an ensemble learning algorithm that builds multiple decision trees during training and outputs the average of the predictions from all the trees. This approach increases accuracy and reduces the risk of overfitting compared to a single decision tree. One of the key advantages of Random Forest is its ability to provide insights into feature importance, which helps us understand which variables are most influential in predicting Walmart's weekly sales.\n",
        "\n",
        "To explain the model’s behavior, we used feature importance analysis, which is built into the Random Forest model itself. This technique ranks features based on how much they decrease the impurity (or error) across all trees in the forest. In our analysis, the most important features turned out to be 'Store', 'CPI' (Consumer Price Index), 'Unemployment', and 'Holiday_Flag'. This indicates that both economic conditions and calendar effects significantly influence sales. Additionally, time-based features like 'Month' and 'Week' also showed meaningful contributions, highlighting the importance of seasonal patterns and recurring sales trends.\n",
        "\n",
        "For more detailed explainability, we can optionally use advanced tools like SHAP (SHapley Additive exPlanations) or LIME to visualize and interpret the model’s predictions at a local and global level. These tools help decompose predictions into individual feature contributions, making the model more transparent and business-friendly. However, for this project, Random Forest’s built-in feature importance provided a sufficient and interpretable way to understand how different factors drive Walmart's weekly sales predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Feature Importance - Random Forest Regressor\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the data table\n",
        "feature_importance_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "IPgnwEhWn4wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**:\n",
        "Store has by far the highest impact, showing store-level differences dominate sales behavior.\n",
        "\n",
        "CPI and Unemployment are strong economic indicators, directly tied to customer spending.\n",
        "\n",
        "Week contributes to capturing seasonal and promotional patterns.\n",
        "\n",
        "Time-based variables like Month, Year, and Season have minor contributions individually, but still help model seasonality."
      ],
      "metadata": {
        "id": "Z6HAyOEsoZsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save the trained model to a .joblib file\n",
        "joblib.dump(rf, 'random_forest_model.joblib')\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('random_forest_model.joblib')\n",
        "\n",
        "# Predict on test set\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "# Show a few predictions vs actual\n",
        "for i in range(5):\n",
        "    print(f\"Predicted: {predictions[i]:,.2f} | Actual: {y_test.iloc[i]:,.2f}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aimed to build a predictive model to accurately forecast Walmart’s weekly sales at the store level, using historical sales data along with external factors such as holidays, temperature, fuel prices, unemployment, and the Consumer Price Index (CPI). Effective sales forecasting is critical for Walmart’s operations—impacting inventory management, workforce allocation, and strategic decision-making.\n",
        "\n",
        "We began by cleaning and preprocessing the dataset, including handling missing values, removing duplicates, converting date fields into meaningful time-based features (e.g., Month, Week, Season), and encoding categorical variables. We then conducted exploratory data analysis (EDA) to identify trends and patterns in the data, such as the significant sales spikes during holiday weeks, variation across stores, and the correlation between economic indicators (like CPI and Unemployment) and weekly sales.\n",
        "\n",
        "Three machine learning models were implemented and evaluated:\n",
        "\n",
        "-Linear Regression as a baseline model\n",
        "\n",
        "-Decision Tree Regressor to capture non-linear patterns\n",
        "\n",
        "-Random Forest Regressor as a more powerful ensemble model\n",
        "\n",
        "After evaluating all models using metrics like RMSE, MAE, and R² score, the Random Forest Regressor was selected as the final model due to its superior accuracy and robustness. We further improved its performance through RandomizedSearchCV, a hyperparameter optimization technique that significantly reduced model error and improved generalization. The trained model was then saved using joblib, reloaded, and tested successfully on unseen data—demonstrating readiness for deployment.\n",
        "\n",
        "**Insights Gained**\n",
        "\n",
        "-Store identity was the most important feature, indicating that location-specific factors heavily influence sales.\n",
        "\n",
        "-CPI and Unemployment were significant, showing the effect of economic conditions on consumer behavior.\n",
        "\n",
        "-Holidays and seasonal patterns (captured through Week and Month) had noticeable impacts on sales volume.\n",
        "\n",
        "-Time-based features provided meaningful lift to model accuracy, highlighting the importance of seasonality in retail sales.\n",
        "\n",
        "**Business Impact**\n",
        "\n",
        "-The predictive model provides Walmart with an accurate, data-driven way to forecast weekly sales.\n",
        "\n",
        "-By identifying high-impact features (like holidays and economic factors), the model enables smarter inventory and workforce planning.\n",
        "\n",
        "-Reducing forecasting errors by even 5–10% can result in millions of dollars in cost savings across supply chain operations.\n",
        "\n",
        "-Reliable predictions will help Walmart minimize stockouts and overstocking, leading to better customer experience and higher revenue.\n",
        "\n",
        "**Final Note**\n",
        "\n",
        "This project demonstrates how machine learning can transform historical retail data into actionable insights and strategic decisions. By deploying the Random Forest model into Walmart’s forecasting pipeline, the company can enhance its operational efficiency, make informed business decisions, and stay competitive in a fast-paced retail environment."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}